{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Most of the time, you’ll need more than one step in your job. To define multiple steps, override steps() and return a list of mrjob.step.MRStep.`__\n",
    "\n",
    "__`Here’s a job that finds the most commonly used word in the input:`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load most_used.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/most_used.root.20160322.165841.848541\n",
      "writing wrapper script to /tmp/most_used.root.20160322.165841.848541/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/most_used.root.20160322.165841.848541/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar2443486007967025815/] [] /tmp/streamjob1360210150763941474.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1458650496725_0005\n",
      "HADOOP: Submitted application application_1458650496725_0005\n",
      "HADOOP: The url to track the job: http://10.211.55.101:8088/proxy/application_1458650496725_0005/\n",
      "HADOOP: Running job: job_1458650496725_0005\n",
      "HADOOP: Job job_1458650496725_0005 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 67% reduce 0%\n",
      "HADOOP: Task Id : attempt_1458650496725_0005_m_000001_0, Status : FAILED\n",
      "HADOOP: Container [pid=10065,containerID=container_1458650496725_0005_01_000003] is running beyond virtual memory limits. Current usage: 216.3 MB of 1 GB physical memory used; 2.2 GB of 2.1 GB virtual memory used. Killing container.\n",
      "HADOOP: Dump of the process-tree for container_1458650496725_0005_01_000003 :\n",
      "HADOOP: \t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "HADOOP: \t|- 10082 10065 10065 10065 (java) 502 21 1951842304 51904 /usr/local/java/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1458650496725_0005/container_1458650496725_0005_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.211.55.101 53007 attempt_1458650496725_0005_m_000001_0 3 \n",
      "HADOOP: \t|- 10174 10165 10065 10065 (python) 178 8 199118848 2704 python most_used.py --step-num=0 --combiner \n",
      "HADOOP: \t|- 10165 10082 10065 10065 (sh) 0 0 108929024 441 /bin/sh -ex setup-wrapper.sh python most_used.py --step-num=0 --combiner \n",
      "HADOOP: \t|- 10065 10063 10065 10065 (bash) 0 0 108613632 331 /bin/bash -c /usr/local/java/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1458650496725_0005/container_1458650496725_0005_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.211.55.101 53007 attempt_1458650496725_0005_m_000001_0 3 1>/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000003/stdout 2>/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000003/stderr  \n",
      "HADOOP: \n",
      "HADOOP: Container killed on request. Exit code is 143\n",
      "HADOOP: Container exited with a non-zero exit code 143\n",
      "HADOOP: \n",
      "HADOOP: Task Id : attempt_1458650496725_0005_m_000000_0, Status : FAILED\n",
      "HADOOP: Container [pid=10069,containerID=container_1458650496725_0005_01_000002] is running beyond virtual memory limits. Current usage: 214.9 MB of 1 GB physical memory used; 2.2 GB of 2.1 GB virtual memory used. Killing container.\n",
      "HADOOP: Dump of the process-tree for container_1458650496725_0005_01_000002 :\n",
      "HADOOP: \t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "HADOOP: \t|- 10083 10069 10069 10069 (java) 488 23 1951842304 51551 /usr/local/java/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1458650496725_0005/container_1458650496725_0005_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.211.55.101 53007 attempt_1458650496725_0005_m_000000_0 2 \n",
      "HADOOP: \t|- 10175 10168 10069 10069 (python) 177 8 199229440 2700 python most_used.py --step-num=0 --combiner \n",
      "HADOOP: \t|- 10069 10067 10069 10069 (bash) 0 0 108613632 332 /bin/bash -c /usr/local/java/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1458650496725_0005/container_1458650496725_0005_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 10.211.55.101 53007 attempt_1458650496725_0005_m_000000_0 2 1>/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000002/stdout 2>/usr/local/hadoop/logs/userlogs/application_1458650496725_0005/container_1458650496725_0005_01_000002/stderr  \n",
      "HADOOP: \t|- 10168 10083 10069 10069 (sh) 0 0 108929024 441 /bin/sh -ex setup-wrapper.sh python most_used.py --step-num=0 --combiner \n",
      "HADOOP: \n",
      "HADOOP: Container killed on request. Exit code is 143\n",
      "HADOOP: Container exited with a non-zero exit code 143\n",
      "HADOOP: \n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 33% reduce 0%\n",
      "HADOOP:  map 67% reduce 0%\n",
      "HADOOP:  map 83% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1458650496725_0005 completed successfully\n",
      "HADOOP: Counters: 51\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=571620\n",
      "HADOOP: \t\tFILE: Number of bytes written=1511297\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=1577433\n",
      "HADOOP: \t\tHDFS: Number of bytes written=636267\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tFailed map tasks=2\n",
      "HADOOP: \t\tLaunched map tasks=4\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tOther local map tasks=2\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=66783\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=5823\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=66783\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=5823\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all map tasks=66783\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all reduce tasks=5823\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all map tasks=68385792\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all reduce tasks=5962752\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=33055\n",
      "HADOOP: \t\tMap output records=271356\n",
      "HADOOP: \t\tMap output bytes=2565104\n",
      "HADOOP: \t\tMap output materialized bytes=571626\n",
      "HADOOP: \t\tInput split bytes=186\n",
      "HADOOP: \t\tCombine input records=271356\n",
      "HADOOP: \t\tCombine output records=39823\n",
      "HADOOP: \t\tReduce input groups=30894\n",
      "HADOOP: \t\tReduce shuffle bytes=571626\n",
      "HADOOP: \t\tReduce input records=39823\n",
      "HADOOP: \t\tReduce output records=30894\n",
      "HADOOP: \t\tSpilled Records=79646\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=294\n",
      "HADOOP: \t\tCPU time spent (ms)=5650\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=537714688\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=6187843584\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=378470400\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=1577247\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=636267\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/most_used.root.20160322.165841.848541/step-output/1\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar7675575271840375998/] [] /tmp/streamjob7576657302107441251.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1458650496725_0006\n",
      "HADOOP: Submitted application application_1458650496725_0006\n",
      "HADOOP: The url to track the job: http://10.211.55.101:8088/proxy/application_1458650496725_0006/\n",
      "HADOOP: Running job: job_1458650496725_0006\n",
      "HADOOP: Job job_1458650496725_0006 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1458650496725_0006 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=698061\n",
      "HADOOP: \t\tFILE: Number of bytes written=1762991\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=640673\n",
      "HADOOP: \t\tHDFS: Number of bytes written=12\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=22164\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=6142\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=22164\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=6142\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all map tasks=22164\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all reduce tasks=6142\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all map tasks=22695936\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all reduce tasks=6289408\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=30894\n",
      "HADOOP: \t\tMap output records=30894\n",
      "HADOOP: \t\tMap output bytes=636267\n",
      "HADOOP: \t\tMap output materialized bytes=698067\n",
      "HADOOP: \t\tInput split bytes=310\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=1\n",
      "HADOOP: \t\tReduce shuffle bytes=698067\n",
      "HADOOP: \t\tReduce input records=30894\n",
      "HADOOP: \t\tReduce output records=1\n",
      "HADOOP: \t\tSpilled Records=61788\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=355\n",
      "HADOOP: \t\tCPU time spent (ms)=2360\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=532979712\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=6187573248\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=378470400\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=640363\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=12\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/most_used.root.20160322.165841.848541/output\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/most_used.root.20160322.165841.848541/output\n",
      "15072\t\"the\"\n",
      "STDERR: 16/03/22 17:00:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /tmp/most_used.root.20160322.165841.848541\n",
      "deleting hdfs:///user/root/tmp/mrjob/most_used.root.20160322.165841.848541 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python most_used.py -r hadoop  hdfs:///user/ashu/pg4300.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
