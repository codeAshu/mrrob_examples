{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p /root/python_mr/data\n",
    "!hdfs dfs -mkdir /user/ashu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://www.gutenberg.org/cache/epub/4300/pg4300.txt\n",
    "!wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put /root/python_mr/data/* /user/ashu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  data.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load count_mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load count_reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    if word == current_word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%i' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%i' % (current_word, current_count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x count_*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\t1\r\n",
      "foo\t1\r\n",
      "quux\t1\r\n",
      "labs\t1\r\n",
      "foo\t1\r\n",
      "bar\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | /root/python_mr/count_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | /root/python_mr/count_mapper.py |sort | /root/python_mr/count_reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/21 18:45:00 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/03/21 18:45:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/root/python_mr/count_mapper.py, /root/python_mr/count_reducer.py, /tmp/hadoop-unjar5148548744413442148/] [] /tmp/streamjob3491843196971813965.jar tmpDir=null\n",
      "16/03/21 18:45:02 INFO client.RMProxy: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "16/03/21 18:45:02 INFO client.RMProxy: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "16/03/21 18:45:03 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/03/21 18:45:03 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/03/21 18:45:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1458581249607_0007\n",
      "16/03/21 18:45:04 INFO impl.YarnClientImpl: Submitted application application_1458581249607_0007\n",
      "16/03/21 18:45:04 INFO mapreduce.Job: The url to track the job: http://10.211.55.101:8088/proxy/application_1458581249607_0007/\n",
      "16/03/21 18:45:04 INFO mapreduce.Job: Running job: job_1458581249607_0007\n",
      "16/03/21 18:45:13 INFO mapreduce.Job: Job job_1458581249607_0007 running in uber mode : false\n",
      "16/03/21 18:45:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/03/21 18:45:29 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/03/21 18:45:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/03/21 18:45:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/03/21 18:45:38 INFO mapreduce.Job: Job job_1458581249607_0007 completed successfully\n",
      "16/03/21 18:45:38 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3178129\n",
      "\t\tFILE: Number of bytes written=6839295\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2106154\n",
      "\t\tHDFS: Number of bytes written=1078723\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40581\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5571\n",
      "\t\tTotal time spent by all map tasks (ms)=40581\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5571\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=40581\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5571\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=41554944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5704704\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=37103\n",
      "\t\tMap output records=279588\n",
      "\t\tMap output bytes=2618221\n",
      "\t\tMap output materialized bytes=3178141\n",
      "\t\tInput split bytes=279\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=61573\n",
      "\t\tReduce shuffle bytes=3178141\n",
      "\t\tReduce input records=279588\n",
      "\t\tReduce output records=61573\n",
      "\t\tSpilled Records=559176\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=657\n",
      "\t\tCPU time spent (ms)=4640\n",
      "\t\tPhysical memory (bytes) snapshot=761217024\n",
      "\t\tVirtual memory (bytes) snapshot=8248160256\n",
      "\t\tTotal committed heap usage (bytes)=537264128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2105875\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1078723\n",
      "16/03/21 18:45:38 INFO streaming.StreamJob: Output directory: /user/ashu/book-output\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-file /root/python_mr/count_mapper.py -mapper /root/python_mr/count_mapper.py \\\n",
    "-file  /root/python_mr/count_reducer.py -reducer  /root/python_mr/count_reducer.py \\\n",
    "-input /user/ashu/* \\\n",
    "-output /user/ashu/book-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/21 18:46:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup          0 2016-03-21 18:45 /user/ashu/book-output/_SUCCESS\n",
      "-rw-r--r--   3 root supergroup      1.0 M 2016-03-21 18:45 /user/ashu/book-output/part-00000\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls -h /user/ashu/book-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/21 18:44:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/21 18:44:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/ashu/book-output\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/ashu/book-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat /user/irmak/book-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference\n",
    "\n",
    "[blog](http://www.quuxlabs.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)<br>\n",
    "[github](https://github.com/codeAshu/hadoop-python-hive-tutorial/blob/master/mapreduce.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
