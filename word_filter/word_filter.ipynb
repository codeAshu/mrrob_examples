{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "word = \"something\"\n",
    "if word.startswith('some'):\n",
    "    print 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load count_mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        \n",
    "        #filter word based on your choice\n",
    "        #lets filter words which start with \"some\"\n",
    "        key = 'some'\n",
    "        if word.startswith(key):\n",
    "            print '%s\\t%s' % (key, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load count_reducer.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t')\n",
    "    count = int(count)\n",
    "    if word == current_word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%i' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%i' % (current_word, current_count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x count_*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\t1\r\n",
      "some\t1\r\n",
      "some\t1\r\n",
      "some\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"sometime we do something while some people think its someone else\" | /root/python_mr/word_filter/count_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\t4\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"sometime we do something while some people think its someone else\" | /root/python_mr/word_filter/count_mapper.py |sort | /root/python_mr/word_filter/count_reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/22 15:28:42 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/03/22 15:28:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/root/python_mr/word_filter/count_mapper.py, /root/python_mr/word_filter/count_reducer.py, /tmp/hadoop-unjar7226349520756563393/] [] /tmp/streamjob689894239602994728.jar tmpDir=null\n",
      "16/03/22 15:28:44 INFO client.RMProxy: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "16/03/22 15:28:45 INFO client.RMProxy: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "16/03/22 15:28:46 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/03/22 15:28:46 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/03/22 15:28:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1458650496725_0003\n",
      "16/03/22 15:28:47 INFO impl.YarnClientImpl: Submitted application application_1458650496725_0003\n",
      "16/03/22 15:28:47 INFO mapreduce.Job: The url to track the job: http://10.211.55.101:8088/proxy/application_1458650496725_0003/\n",
      "16/03/22 15:28:47 INFO mapreduce.Job: Running job: job_1458650496725_0003\n",
      "16/03/22 15:28:59 INFO mapreduce.Job: Job job_1458650496725_0003 running in uber mode : false\n",
      "16/03/22 15:28:59 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/03/22 15:29:12 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/03/22 15:29:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/03/22 15:29:21 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/03/22 15:29:21 INFO mapreduce.Job: Job job_1458650496725_0003 completed successfully\n",
      "16/03/22 15:29:22 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5847\n",
      "\t\tFILE: Number of bytes written=494843\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3180793\n",
      "\t\tHDFS: Number of bytes written=9\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=34330\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5944\n",
      "\t\tTotal time spent by all map tasks (ms)=34330\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5944\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=34330\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5944\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=35153920\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6086656\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=98676\n",
      "\t\tMap output records=649\n",
      "\t\tMap output bytes=4543\n",
      "\t\tMap output materialized bytes=5859\n",
      "\t\tInput split bytes=291\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=5859\n",
      "\t\tReduce input records=649\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1298\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=578\n",
      "\t\tCPU time spent (ms)=2450\n",
      "\t\tPhysical memory (bytes) snapshot=746270720\n",
      "\t\tVirtual memory (bytes) snapshot=8247881728\n",
      "\t\tTotal committed heap usage (bytes)=537264128\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3180502\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9\n",
      "16/03/22 15:29:22 INFO streaming.StreamJob: Output directory: /user/ashu/filter-output\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "-file /root/python_mr/word_filter/count_mapper.py -mapper /root/python_mr/word_filter/count_mapper.py \\\n",
    "-file  /root/python_mr/word_filter/count_reducer.py -reducer  /root/python_mr/word_filter/count_reducer.py \\\n",
    "-input /user/ashu/* \\\n",
    "-output /user/ashu/filter-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls -h /user/ashu/filter-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -rm -r /user/ashu/filter-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat /user/ashu/filter-output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
