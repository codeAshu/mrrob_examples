{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/\r\n"
     ]
    }
   ],
   "source": [
    "!echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load word_count.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /tmp/word_count.root.20160322.155628.645388\r\n",
      "\r\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\r\n",
      "\r\n",
      "writing to /tmp/word_count.root.20160322.155628.645388/step-0-mapper_part-00000\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "writing to /tmp/word_count.root.20160322.155628.645388/step-0-mapper-sorted\r\n",
      "> sort /tmp/word_count.root.20160322.155628.645388/step-0-mapper_part-00000\r\n",
      "writing to /tmp/word_count.root.20160322.155628.645388/step-0-reducer_part-00000\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "Moving /tmp/word_count.root.20160322.155628.645388/step-0-reducer_part-00000 -> /tmp/word_count.root.20160322.155628.645388/output/part-00000\r\n",
      "Streaming final output from /tmp/word_count.root.20160322.155628.645388/output\r\n",
      "\"chars\"\t20976\r\n",
      "\"lines\"\t95\r\n",
      "\"words\"\t1109\r\n",
      "removing tmp directory /tmp/word_count.root.20160322.155628.645388\r\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py my_file.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/word_count.root.20160322.163243.953559\n",
      "writing wrapper script to /tmp/word_count.root.20160322.163243.953559/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/word_count.root.20160322.163243.953559/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar1331004643591061974/] [] /tmp/streamjob5555327017865886134.jar tmpDir=null\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Connecting to ResourceManager at /10.211.55.101:8032\n",
      "HADOOP: Total input paths to process : 4\n",
      "HADOOP: number of splits:4\n",
      "HADOOP: Submitting tokens for job: job_1458650496725_0004\n",
      "HADOOP: Submitted application application_1458650496725_0004\n",
      "HADOOP: The url to track the job: http://10.211.55.101:8088/proxy/application_1458650496725_0004/\n",
      "HADOOP: Running job: job_1458650496725_0004\n",
      "HADOOP: Job job_1458650496725_0004 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 25% reduce 0%\n",
      "HADOOP:  map 75% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 67%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1458650496725_0004 completed successfully\n",
      "HADOOP: Counters: 50\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=3635463\n",
      "HADOOP: \t\tFILE: Number of bytes written=7882504\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=3180909\n",
      "HADOOP: \t\tHDFS: Number of bytes written=45\n",
      "HADOOP: \t\tHDFS: Number of read operations=15\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tKilled map tasks=1\n",
      "HADOOP: \t\tLaunched map tasks=4\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=4\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=120951\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=38315\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=120951\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=38315\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all map tasks=120951\n",
      "HADOOP: \t\tTotal vcore-milliseconds taken by all reduce tasks=38315\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all map tasks=123853824\n",
      "HADOOP: \t\tTotal megabyte-milliseconds taken by all reduce tasks=39234560\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=98677\n",
      "HADOOP: \t\tMap output records=296031\n",
      "HADOOP: \t\tMap output bytes=3043395\n",
      "HADOOP: \t\tMap output materialized bytes=3635481\n",
      "HADOOP: \t\tInput split bytes=398\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=3\n",
      "HADOOP: \t\tReduce shuffle bytes=3635481\n",
      "HADOOP: \t\tReduce input records=296031\n",
      "HADOOP: \t\tReduce output records=3\n",
      "HADOOP: \t\tSpilled Records=592062\n",
      "HADOOP: \t\tShuffled Maps =4\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=4\n",
      "HADOOP: \t\tGC time elapsed (ms)=409\n",
      "HADOOP: \t\tCPU time spent (ms)=5260\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=956272640\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=10305654784\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=696057856\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=3180511\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=45\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/word_count.root.20160322.163243.953559/output\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/word_count.root.20160322.163243.953559/output\n",
      "\"chars\"\t3048764\n",
      "\"lines\"\t98677\n",
      "\"words\"\t402736\n",
      "STDERR: 16/03/22 16:36:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /tmp/word_count.root.20160322.163243.953559\n",
      "deleting hdfs:///user/root/tmp/mrjob/word_count.root.20160322.163243.953559 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py -r hadoop  hdfs:///user/ashu/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python word_count.py -r hadoop  hdfs:///user/ashu/pg4300.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
